{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c986d23",
   "metadata": {},
   "source": [
    "# Mini-Project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd5882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api  # load \n",
    "from gensim.similarities import Similarity #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff65f1",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40d26a",
   "metadata": {},
   "source": [
    "#### Load the word2vec-google-news-300 pretrained embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "642be64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_300 = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9492f4a",
   "metadata": {},
   "source": [
    "#### Loading synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b533ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = pd.read_csv(\"synonyms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47cb89de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our own test case\n",
    "synonyms1 = pd.read_csv(\"synonyms1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf61da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enormously</td>\n",
       "      <td>tremendously</td>\n",
       "      <td>appropriately</td>\n",
       "      <td>uniquely</td>\n",
       "      <td>tremendously</td>\n",
       "      <td>decidedly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>provisions</td>\n",
       "      <td>stipulations</td>\n",
       "      <td>stipulations</td>\n",
       "      <td>interrelations</td>\n",
       "      <td>jurisdictions</td>\n",
       "      <td>interpretations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>haphazardly</td>\n",
       "      <td>randomly</td>\n",
       "      <td>dangerously</td>\n",
       "      <td>densely</td>\n",
       "      <td>randomly</td>\n",
       "      <td>linearly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prominent</td>\n",
       "      <td>conspicuous</td>\n",
       "      <td>battered</td>\n",
       "      <td>ancient</td>\n",
       "      <td>mysterious</td>\n",
       "      <td>conspicuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zenith</td>\n",
       "      <td>pinnacle</td>\n",
       "      <td>completion</td>\n",
       "      <td>pinnacle</td>\n",
       "      <td>outset</td>\n",
       "      <td>decline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      question        answer              0               1              2  \\\n",
       "0   enormously  tremendously  appropriately        uniquely   tremendously   \n",
       "1   provisions  stipulations   stipulations  interrelations  jurisdictions   \n",
       "2  haphazardly      randomly    dangerously         densely       randomly   \n",
       "3    prominent   conspicuous       battered         ancient     mysterious   \n",
       "4       zenith      pinnacle     completion        pinnacle         outset   \n",
       "\n",
       "                 3  \n",
       "0        decidedly  \n",
       "1  interpretations  \n",
       "2         linearly  \n",
       "3      conspicuous  \n",
       "4          decline  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbf030",
   "metadata": {},
   "source": [
    "#### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a225b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab  \n",
    "vocab = word2vec_300.index_to_key "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3b128",
   "metadata": {},
   "source": [
    "#### Create output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f20893ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## file\n",
    "f = open(\"word2vec-google-news-300-details.csv\", \"w\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a4ccf7",
   "metadata": {},
   "source": [
    "## Some test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf13a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vehicle', 0.7821096181869507),\n",
       " ('cars', 0.7423830032348633),\n",
       " ('SUV', 0.7160962224006653),\n",
       " ('minivan', 0.6907036304473877),\n",
       " ('truck', 0.6735789775848389),\n",
       " ('Car', 0.6677608489990234),\n",
       " ('Ford_Focus', 0.667320191860199),\n",
       " ('Honda_Civic', 0.6626849174499512),\n",
       " ('Jeep', 0.651133120059967),\n",
       " ('pickup_truck', 0.6441438794136047)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_300.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce05eb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_300.similarity('car', 'car'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c0e28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43913448"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_300.similarity('car', 'Auto'  ) #?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94678917",
   "metadata": {},
   "source": [
    "## Task 1 ( Evaluation of the word2vec-google-news-300 Pre-trained Model )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c506517",
   "metadata": {},
   "source": [
    "### Part 1 of task 1 ( predict the synonym and append the result of questions to csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c675b6ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enormously,tremendously,tremendously,correct\n",
      "provisions,stipulations,stipulations,correct\n",
      "haphazardly,randomly,randomly,correct\n",
      "prominent,conspicuous,conspicuous,correct\n",
      "zenith,pinnacle,pinnacle,correct\n",
      "flawed,imperfect,imperfect,correct\n",
      "urgently,desperately,desperately,correct\n",
      "consumed,eaten,eaten,correct\n",
      "advent,coming,coming,correct\n",
      "concisely,succinctly,succinctly,correct\n",
      "salutes,greetings,ceremonies,wrong\n",
      "solitary,alone,restless,wrong\n",
      "hasten,accelerate,accelerate,correct\n",
      "perseverance,endurance,generosity,wrong\n",
      "fanciful,imaginative,imaginative,correct\n",
      "showed,demonstrated,demonstrated,correct\n",
      "constantly,continually,continually,correct\n",
      "issues,subjects,subjects,correct\n",
      "furnish,supply,impress,wrong\n",
      "costly,expensive,expensive,correct\n",
      "recognized,acknowledged,acknowledged,correct\n",
      "spot,location,location,correct\n",
      "make,earn,earn,correct\n",
      "often,frequently,frequently,correct\n",
      "easygoing,relaxed,relaxed,correct\n",
      "debate,argument,argument,correct\n",
      "narrow,thin,thin,correct\n",
      "arranged,planned,planned,correct\n",
      "infinite,limitless,limitless,correct\n",
      "showy,striking,prickly,wrong\n",
      "levied,imposed,imposed,correct\n",
      "deftly,skillfully,skillfully,correct\n",
      "distribute,circulate,commercialize,wrong\n",
      "discrepancies,differences,differences,correct\n",
      "prolific,productive,productive,correct\n",
      "unmatched,unequaled,unequaled,correct\n",
      "peculiarly,uniquely,uniquely,correct\n",
      "hue,color,color,correct\n",
      "hind,rear,rear,correct\n",
      "highlight,accentuate,accentuate,correct\n",
      "hastily,hurriedly,hurriedly,correct\n",
      "temperate,mild,mild,correct\n",
      "grin,smile,smile,correct\n",
      "verbally,orally,orally,correct\n",
      "physician,doctor,doctor,correct\n",
      "essentially,basically,basically,correct\n",
      "keen,sharp,useful,wrong\n",
      "situated,positioned,positioned,correct\n",
      "principal,major,major,correct\n",
      "slowly,gradually,gradually,correct\n",
      "built,constructed,constructed,correct\n",
      "tasks,jobs,jobs,correct\n",
      "unlikely,improbable,improbable,correct\n",
      "halfheartedly,apathetically,apathetically,correct\n",
      "annals,chronicles,chronicles,correct\n",
      "wildly,furiously,furiously,correct\n",
      "hailed,acclaimed,remembered,wrong\n",
      "command,mastery,mastery,correct\n",
      "concocted,devised,devised,correct\n",
      "prospective,potential,potential,correct\n",
      "generally,broadly,broadly,correct\n",
      "sustained,prolonged,prolonged,correct\n",
      "perilous,dangerous,dangerous,correct\n",
      "tranquillity,peacefulness,weariness,guess\n",
      "dissipate,disperse,disperse,correct\n",
      "primarily,chiefly,chiefly,correct\n",
      "colloquial,conversational,conversational,correct\n",
      "resolved,settled,settled,correct\n",
      "feasible,possible,possible,correct\n",
      "expeditiously,rapidly,rapidly,correct\n",
      "percentage,proportion,proportion,correct\n",
      "terminated,ended,postponed,wrong\n",
      "uniform,alike,alike,correct\n",
      "figure,solve,solve,correct\n",
      "sufficient,enough,enough,correct\n",
      "fashion,manner,manner,correct\n",
      "marketed,sold,sold,correct\n",
      "bigger,larger,larger,correct\n",
      "roots,origins,origins,correct\n",
      "normally,ordinarily,ordinarily,correct\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_guess = 0\n",
    "\n",
    "f = open(\"word2vec-google-news-300-details.csv\", \"a\")\n",
    "for i, row in synonyms.iterrows():\n",
    "    answer = row[1]\n",
    "    question = row[0]\n",
    "    predict = row[(random.randint(2, 5))]\n",
    "    if(row[0] in vocab):\n",
    "        q= np.array( [word2vec_300.similarity(question,  word ) if word in vocab else -1000 for word in (row[2:])] )\n",
    "        max =np.argmax(q)\n",
    "        if (q[max] != -1000):\n",
    "                predict = row[max+2]\n",
    "                if predict == answer:\n",
    "                    total_correct += 1\n",
    "                    f.write(question+\",\"+answer+\",\"+predict+\",correct\\n\")\n",
    "                    print(f'{question},{answer},{predict},correct')\n",
    "                else:\n",
    "                    f.write(question+\",\"+answer+\",\"+predict+\",wrong\\n\")\n",
    "                    print(f'{question},{answer},{predict},wrong')\n",
    "        else:\n",
    "            total_guess += 1\n",
    "            f.write(question+\",\"+answer+\",\"+predict+\",guess\\n\")\n",
    "            print(f'{question},{answer},{predict},guess')\n",
    "            \n",
    "    else:\n",
    "        total_guess += 1\n",
    "        f.write(question+\",\"+answer+\",\"+predict+\",guess\\n\")\n",
    "        print(f'{question},{answer},{predict},guess')\n",
    "                \n",
    "        \n",
    "f.close()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a4ec3",
   "metadata": {},
   "source": [
    "I comented out the other stuff , we can remove them \n",
    "i let you guys remove them :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9686609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synonyms.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537b29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b4e69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"word2vec-google-news-300-details.csv\", \"a\")\n",
    "\n",
    "# rows = synonyms.shape[0]\n",
    "# columns = synonyms.shape[1]\n",
    "# label = \"\"\n",
    "# word_result = \"\"\n",
    "# for row in range(0, synonyms.shape[0]):\n",
    "#     print(f'Question word :{synonyms.iloc[row,:][0]}')\n",
    "#     question_word = {synonyms.iloc[row,:][0]}\n",
    "#     f.write(repr(question_word) + \",\")\n",
    "#     max = 0\n",
    "    \n",
    "#     # Check if it is a guess \n",
    "    \n",
    "    \n",
    "    \n",
    "#     #if (synonyms.iloc[row, :][0] in vocab) or ( (synonyms.iloc[row, :][2] not in vocab) and (synonyms.iloc[row, :][3] not in vocab) and (synonyms.iloc[row, :][4] not in vocab) and (synonyms.iloc[row, :][5] not in vocab) ):\n",
    "#         #label = \"guess\"\n",
    "    \n",
    "#     # Makes a boolean array where each element is the boolean result of checking if the word is in the model\n",
    "    \n",
    "#     word_boolean_list = list()\n",
    "#     for i in range(0,6):\n",
    "#         word_boolean_list.append(synonyms.iloc[row, :][i] in vocab)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "#     for word in range(2, synonyms.shape[1]):\n",
    "#         #print(f'Question :{synonyms.iloc[row,:][0]} ==> Answer: {synonyms.iloc[row,:][word]}')  \n",
    "#         #correct_word = {synonyms.iloc[row,:][word]}\n",
    "#         #f.write(repr(correct_word) + \",\")\n",
    "        \n",
    "        \n",
    "        \n",
    "# #         max = 0.2\n",
    "# #         if(word > max)\n",
    "# #         max =\n",
    "#         # if question word present in model\n",
    "#         if synonyms.iloc[row,:][0] in vocab:\n",
    "#             cosine_value = word2vec_300.similarity(synonyms.iloc[row,:][0], synonyms.iloc[row,:][word])\n",
    "#         if cosine_value > max:\n",
    "#             max = cosine_value\n",
    "#             word_result = {synonyms.iloc[row,:][word]}\n",
    "#         #print(word2vec_300.similarity(synonyms.iloc[row,:][0], synonyms.iloc[row,:][word]  ))\n",
    "#     print(\"THE WORD FOUND IS: \" + repr(word_result))\n",
    "    \n",
    "#     # Creating a label\n",
    "    \n",
    "#     for i in word_result:\n",
    "#         guess = str(i)\n",
    "    \n",
    "#     print(guess)\n",
    "    \n",
    "#     print(''.join(word_result))\n",
    "    \n",
    "    \n",
    "#     # If it is a guess\n",
    "    \n",
    "#     if( (word_boolean_list[0] == False) or ( (word_boolean_list[2] == False) and (word_boolean_list[3] == False) and (word_boolean_list[4] == False) and (word_boolean_list[5] == False) ) ):\n",
    "#         label = \"guess\"\n",
    "#     elif( (word_boolean_list[0] == True) or ( (word_boolean_list[2] == True) or (word_boolean_list[3] == True) or (word_boolean_list[4] == True) or (word_boolean_list[5] == True) ) ):\n",
    "#         if(guess == synonyms.iloc[row,:][1]):\n",
    "#             label = \"correct\"\n",
    "#         else:\n",
    "#             label = \"wrong\"\n",
    "#     print(label)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b593963c",
   "metadata": {},
   "source": [
    "## Task 1 part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227bc13",
   "metadata": {},
   "source": [
    "### Model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9580deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"analysis.csv\", \"w\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b8080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"word2vec-google-news-300\"\n",
    "vector_size = len(word2vec_300[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9095",
   "metadata": {},
   "source": [
    "### Size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af805e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa30a24",
   "metadata": {},
   "source": [
    "### Number of correct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86559c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d726af0",
   "metadata": {},
   "source": [
    "### Number of questions answered without guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca279552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "total_no_guess = len(synonyms) - total_guess\n",
    "print(total_no_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82392a",
   "metadata": {},
   "source": [
    "### Accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f330e7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860759493670886\n"
     ]
    }
   ],
   "source": [
    "accuracy = total_correct / total_no_guess\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5689f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"analysis.csv\", \"a\")\n",
    "f.write(repr(model_name)+\",\"+repr(vocab_size)+\",\"+repr(total_correct)+\",\"+repr(total_no_guess)+\",\"+repr(accuracy)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60a837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
